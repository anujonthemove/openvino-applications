{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "from openvino_face_helpers.face_detector import FaceDetector\n",
    "from openvino_face_helpers.utils import OutputTransform\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Get the current working directory and add the parent directory to the Python path\n",
    "current_working_directory = os.getcwd()\n",
    "sys.path.append(os.path.join(current_working_directory, \"..\"))\n",
    "\n",
    "from helpers.helpers import (\n",
    "    VideoPlayer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"[ %(levelname)s ] - %(asctime)s - %(message)s\", level=logging.DEBUG, stream=sys.stdout\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDetectionPipeline:\n",
    "\n",
    "    def __init__(self, fd_model_path):\n",
    "        assert os.path.exists(fd_model_path), f\"Model file not found at {fd_model_path}\"\n",
    "        self.core = Core()\n",
    "        self.device = \"CPU\"\n",
    "        # Specify the input size of detection model for reshaping. Example: 500 700. \n",
    "        # Pass (0, 0) for model default\n",
    "        self.input_image_size = (0, 0)\n",
    "        # detection threshold\n",
    "        self.face_detection_confidence_threshold = 0.5\n",
    "        # Scaling ratio for bboxes passed to face recognition.\n",
    "        self.roi_scale_factor = 1.1\n",
    "        # Specify the maximum output window resolution in (width x height) format. \n",
    "        # Example: 1280x720. Input frame size used by default.\n",
    "        self.output_resolution = None\n",
    "        self.face_detector = FaceDetector(self.core, \n",
    "                                            fd_model_path, \n",
    "                                            self.input_image_size,\n",
    "                                            confidence_threshold=self.face_detection_confidence_threshold,\n",
    "                                            roi_scale_factor=self.roi_scale_factor\n",
    "                                            )\n",
    "        \n",
    "        self.face_detector.deploy(self.device)\n",
    "        self.QUEUE_SIZE = 16\n",
    "        \n",
    "    def process(self, frame):\n",
    "        orig_image = frame.copy()\n",
    "        rois = self.face_detector.infer((frame,))\n",
    "        \n",
    "        if self.QUEUE_SIZE < len(rois):\n",
    "            logging.warning('Too many faces for processing. Will be processed only {self.QUEUE_SIZE} of {len(rois)}')\n",
    "            rois = rois[:self.QUEUE_SIZE]\n",
    "        return rois\n",
    "\n",
    "    def draw_detections(self, frame, detections, output_transform):\n",
    "        size = frame.shape[:2]\n",
    "        frame = output_transform.resize(frame)\n",
    "        for roi in detections:\n",
    "            \n",
    "            xmin = max(int(roi.position[0]), 0)\n",
    "            ymin = max(int(roi.position[1]), 0)\n",
    "            xmax = min(int(roi.position[0] + roi.size[0]), size[1])\n",
    "            ymax = min(int(roi.position[1] + roi.size[1]), size[0])\n",
    "            xmin, ymin, xmax, ymax = output_transform.scale([xmin, ymin, xmax, ymax])\n",
    "            \n",
    "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 220, 0), 2)\n",
    "            \n",
    "            # Extract face region\n",
    "            face_region = frame[ymin:ymax, xmin:xmax]\n",
    "\n",
    "            # Convert face region to grayscale\n",
    "            gray_face = cv2.cvtColor(face_region, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Apply Gaussian blur for smoother heatmap\n",
    "            blurred_heatmap = cv2.GaussianBlur(gray_face, (5, 5), 0)\n",
    "\n",
    "            # Apply colormap for visualization\n",
    "            heatmap_colored = cv2.applyColorMap(blurred_heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "            # Introduce random color shift for each frame\n",
    "            color_shift = np.random.randint(0, 256, 3)\n",
    "            heatmap_colored = heatmap_colored + color_shift\n",
    "\n",
    "            # Ensure that the values stay within the valid range [0, 255]\n",
    "            heatmap_colored = np.clip(heatmap_colored, 0, 255).astype(np.uint8)\n",
    "\n",
    "            # Blend the heatmap with the original frame\n",
    "            blended = cv2.addWeighted(face_region, 0.3, heatmap_colored, 0.7, 0)  # Adjust weights\n",
    "\n",
    "            # Replace the original face region with the blended result\n",
    "            frame[ymin:ymax, xmin:xmax] = blended\n",
    "\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def run_async(self, source=0, required_fps=30, title=\"Face Detection\"):\n",
    "    \n",
    "        frame_number = 0\n",
    "        player = None\n",
    "        \n",
    "        try:\n",
    "            # Create a video player\n",
    "            player = VideoPlayer(source, fps=required_fps)\n",
    "            # Start capturing\n",
    "            start_time = time.time()\n",
    "            player.start()\n",
    "\n",
    "            while True:\n",
    "                frame = player.next()\n",
    "                \n",
    "                if frame is None:\n",
    "                    print(\"Source ended\")\n",
    "                    break\n",
    "                \n",
    "\n",
    "                if frame_number == 0:\n",
    "                    output_transform = OutputTransform(frame.shape[:2], self.output_resolution)\n",
    "                    if self.output_resolution:\n",
    "                        self.output_resolution = output_transform.new_resolution\n",
    "                    else:\n",
    "                        self.output_resolution = (frame.shape[1], frame.shape[0])\n",
    "\n",
    "                detections = self.process(frame)\n",
    "                \n",
    "                stop_time = time.time()\n",
    "                total_time = stop_time - start_time\n",
    "                frame_number += 1\n",
    "                sync_fps = frame_number / total_time\n",
    "                \n",
    "                frame = self.draw_detections(frame, detections, output_transform)\n",
    "                cv2.putText(frame, \"Running Asynchronously\", (5, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 255), 2)\n",
    "                cv2.putText(frame, f\"{round(sync_fps, 2)} FPS\", (5, 60), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 255, 0), 2)\n",
    "                cv2.namedWindow(title, cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "                cv2.imshow(title, frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Processing interrupted by user.\")\n",
    "        finally:\n",
    "            cv2.destroyAllWindows()\n",
    "            if player is not None:\n",
    "                player.stop()\n",
    "\n",
    "\n",
    "    def run_sync(self, source=0):\n",
    "        \n",
    "        cap = cv2.VideoCapture(source)\n",
    "        frame_number = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                \n",
    "                if not ret:\n",
    "                    print(\"Source ended or not found\")\n",
    "                    break\n",
    "                \n",
    "                if frame_number == 0:\n",
    "                    output_transform = OutputTransform(frame.shape[:2], self.output_resolution)\n",
    "                    if self.output_resolution:\n",
    "                        self.output_resolution = output_transform.new_resolution\n",
    "                    else:\n",
    "                        self.output_resolution = (frame.shape[1], frame.shape[0])\n",
    "\n",
    "                detections = self.process(frame)\n",
    "\n",
    "\n",
    "                stop_time = time.time()\n",
    "                total_time = stop_time - start_time\n",
    "                frame_number += 1\n",
    "                sync_fps = frame_number / total_time\n",
    "\n",
    "\n",
    "                frame = self.draw_detections(frame, detections, output_transform)\n",
    "                cv2.putText(frame, \"Running Synchronously\", (5, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 255), 2)\n",
    "                cv2.putText(frame, f\"{round(sync_fps, 2)} FPS\", (5, 60), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 255, 0), 2)\n",
    "                cv2.imshow('Face recognition demo', frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                # Quit\n",
    "                if key in {ord('q'), ord('Q'), 27}:\n",
    "                    break\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Processing interrupted by user.\")\n",
    "        finally:\n",
    "            cv2.destroyAllWindows()\n",
    "            cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_face_detection(source, model_path, required_fps, with_async=True):\n",
    "    obj = FaceDetectionPipeline(model_path)\n",
    "    if with_async:\n",
    "        obj.run_async(source=source, required_fps=required_fps)\n",
    "    else:\n",
    "        obj.run_sync(source=source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    source = 2\n",
    "    fd_model_path = \"/home/acer/workspace/intel_models/intel/face-detection-adas-0001/FP32/face-detection-adas-0001.xml\"\n",
    "    \n",
    "    run_face_detection(source, fd_model_path, required_fps=60, with_async=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] - 2023-12-23 19:29:44,228 - Reading Face Detection model /home/acer/workspace/intel_models/intel/face-detection-adas-0001/FP32/face-detection-adas-0001.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] - 2023-12-23 19:29:44,346 - The Face Detection model /home/acer/workspace/intel_models/intel/face-detection-adas-0001/FP32/face-detection-adas-0001.xml is loaded to CPU\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
