{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Person Detection and Tracking with state machine and audio output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIExSCiAgICBJRExFW0lkbGVdCiAgICBDT09MRE9XTltDb29sZG93bl0KICAgIEhFTExPW0hlbGxvXQoKICAgIElETEUgLS0+IENPT0xET1dOCiAgICBDT09MRE9XTiAtLT4gSEVMTE8KICAgIEhFTExPIC0tPiBDT09MRE9XTgo=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import base64\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mm(graph):\n",
    "    graphbytes = graph.encode(\"utf8\")\n",
    "    base64_bytes = base64.b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))\n",
    "\n",
    "mm(\"\"\"\n",
    "graph LR\n",
    "    IDLE[Idle]\n",
    "    COOLDOWN[Cooldown]\n",
    "    HELLO[Hello]\n",
    "\n",
    "    IDLE --> COOLDOWN\n",
    "    COOLDOWN --> HELLO\n",
    "    HELLO --> COOLDOWN\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import openvino as ov\n",
    "import notebook_utils as utils\n",
    "from deepsort_utils.tracker import Tracker\n",
    "from deepsort_utils.nn_matching import NearestNeighborDistanceMetric\n",
    "from deepsort_utils.detection import Detection, compute_color_for_labels, xywh_to_xyxy, xywh_to_tlwh, tlwh_to_xyxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLoader:\n",
    "    def __init__(self, model_path, core, batchsize=1, device=\"AUTO\"):\n",
    "        self.model = core.read_model(model=model_path)\n",
    "        self.input_layer = self.model.input(0)\n",
    "        self.input_shape = self.input_layer.shape\n",
    "        self.height = self.input_shape[2]\n",
    "        self.width = self.input_shape[3]\n",
    "\n",
    "        for layer in self.model.inputs:\n",
    "            input_shape = layer.partial_shape\n",
    "            input_shape[0] = batchsize\n",
    "            self.model.reshape({layer: input_shape})\n",
    "        self.compiled_model = core.compile_model(model=self.model, device_name=device)\n",
    "        self.output_layer = self.compiled_model.output(0)\n",
    "\n",
    "    def predict(self, input):\n",
    "        result = self.compiled_model(input)[self.output_layer]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame processor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameProcessor:\n",
    "    def preprocess(self, frame, height, width):\n",
    "        resized_image = cv2.resize(frame, (width, height))\n",
    "        resized_image = resized_image.transpose((2, 0, 1))\n",
    "        input_image = np.expand_dims(resized_image, axis=0).astype(np.float32)\n",
    "        return input_image\n",
    "\n",
    "    def batch_preprocess(self, img_crops, height, width):\n",
    "        img_batch = np.concatenate([\n",
    "            self.preprocess(img, height, width)\n",
    "            for img in img_crops\n",
    "        ], axis=0)\n",
    "        return img_batch\n",
    "\n",
    "    def process_results(self, h, w, results, thresh=0.5):\n",
    "        # The 'results' variable is a [1, 1, N, 7] tensor.\n",
    "        detections = results.reshape(-1, 7)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        scores = []\n",
    "        for i, detection in enumerate(detections):\n",
    "            _, label, score, xmin, ymin, xmax, ymax = detection\n",
    "            # Filter detected objects.\n",
    "            if score > thresh:\n",
    "                # Create a box with pixels coordinates from the box with normalized coordinates [0,1].\n",
    "                boxes.append(\n",
    "                    [(xmin + xmax) / 2 * w, (ymin + ymax) / 2 * h, (xmax - xmin) * w, (ymax - ymin) * h]\n",
    "                )\n",
    "                labels.append(int(label))\n",
    "                scores.append(float(score))\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            boxes = np.array([]).reshape(0, 4)\n",
    "            scores = np.array([])\n",
    "            labels = np.array([])\n",
    "        return np.array(boxes), np.array(scores), np.array(labels)\n",
    "\n",
    "    def draw_boxes(self, img, bbox, identities=None):\n",
    "        for i, box in enumerate(bbox):\n",
    "            x1, y1, x2, y2 = [int(i) for i in box]\n",
    "            # box text and bar\n",
    "            id = int(identities[i]) if identities is not None else 0\n",
    "            color = compute_color_for_labels(id)\n",
    "            label = '{}{:d}'.format(\"\", id)\n",
    "            t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 2, 2)[0]\n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.rectangle(\n",
    "                img, (x1, y1), (x1 + t_size[0] + 3, y1 + t_size[1] + 4), color, -1)\n",
    "            cv2.putText(\n",
    "                img,\n",
    "                label,\n",
    "                (x1, y1 + t_size[1] + 4),\n",
    "                cv2.FONT_HERSHEY_PLAIN,\n",
    "                1.6,\n",
    "                [255, 255, 255],\n",
    "                2\n",
    "            )\n",
    "        return img\n",
    "\n",
    "    # def cosin_metric(self, x1, x2):\n",
    "    #     return np.dot(x1, x2) / (np.linalg.norm(x1) * np.linalg.norm(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize tracker function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tracker(\n",
    "    metric_name=\"cosine\", \n",
    "    nn_budget=100, \n",
    "    max_cosine_distance=0.6, \n",
    "    max_iou_distance=0.7,\n",
    "    max_age=70,\n",
    "    n_init=3\n",
    "    ):\n",
    "\n",
    "    metric = NearestNeighborDistanceMetric(metric_name, max_cosine_distance, nn_budget)\n",
    "    tracker = Tracker(\n",
    "        metric,\n",
    "        max_iou_distance=max_iou_distance,\n",
    "        max_age=max_age,\n",
    "        n_init=n_init\n",
    "    )\n",
    "\n",
    "    return tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from playsound import playsound\n",
    "import threading\n",
    "\n",
    "# Specify the path to the existing audio file\n",
    "existing_audio_path = \"/home/acer/workspace/openvino-applications/welcome.mp3\"  # Replace with the actual path\n",
    "\n",
    "\n",
    "class PersonTrackingStateMachine:\n",
    "    def __init__(self):\n",
    "        self.state = \"IDLE\"\n",
    "        self.cooldown_start_time = 0\n",
    "        self.cooldown_duration = 10  # in seconds\n",
    "        self.processed_ids = []\n",
    "        self.sound_thread = None\n",
    "\n",
    "    def start_cooldown_timer(self):\n",
    "        self.cooldown_start_time = time.time()\n",
    "\n",
    "    def update_cooldown_timer(self):\n",
    "        if self.state != \"IDLE\":\n",
    "            elapsed_time = time.time() - self.cooldown_start_time\n",
    "            remaining_time = max(0, self.cooldown_duration - elapsed_time)\n",
    "            # clear_output(wait=True)  # Clear the previous output\n",
    "            print(f\"Cooldown timer: {remaining_time:.2f} seconds\")\n",
    "\n",
    "            if remaining_time == 0:\n",
    "                self.state = \"IDLE\"\n",
    "                print(\"Cooldown timer reached zero. Transitioning to IDLE.\")\n",
    "    \n",
    "    def play_hello_audio(self):\n",
    "        # Using playsound to play the existing audio file\n",
    "        # playsound(existing_audio_path)\n",
    "\n",
    "        # Using threading to play the existing audio file in the background\n",
    "        self.sound_thread = threading.Thread(target=playsound, args=(existing_audio_path,))\n",
    "        self.sound_thread.start()\n",
    "\n",
    "    def stop_sound_thread(self):\n",
    "        if self.sound_thread and self.sound_thread.is_alive():\n",
    "            self.sound_thread.join()\n",
    "\n",
    "\n",
    "    def process_detections(self, new_ids):\n",
    "        if self.state == \"IDLE\":\n",
    "            new_unique_ids = list(set(new_ids) - set(self.processed_ids))\n",
    "            if new_unique_ids:\n",
    "                # clear_output(wait=True)  # Clear the previous output\n",
    "                print(\"Hello\")\n",
    "                self.play_hello_audio()  # Play the existing audio\n",
    "                self.processed_ids.extend(new_unique_ids)\n",
    "                self.start_cooldown_timer()\n",
    "                self.state = \"COOLDOWN\"\n",
    "        elif self.state == \"COOLDOWN\":\n",
    "            new_unique_ids = list(set(new_ids) - set(self.processed_ids))\n",
    "            self.processed_ids.extend(new_unique_ids)\n",
    "        elif self.state == \"HELLO_AFTER_COOLDOWN\":\n",
    "            new_unique_ids = list(set(new_ids) - set(self.processed_ids))\n",
    "            if new_unique_ids:\n",
    "                # clear_output(wait=True)  # Clear the previous output\n",
    "                print(\"Hello\")\n",
    "                self.play_hello_audio()  # Play the existing audio\n",
    "                self.processed_ids.extend(new_unique_ids)\n",
    "                self.start_cooldown_timer()\n",
    "                self.state = \"COOLDOWN\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionPipeline:\n",
    "    def __init__(self, detection_model_path, reid_model_path):\n",
    "        self.core = ov.Core()\n",
    "        self.detector = ModelLoader(model_path=detection_model_path, core=self.core)\n",
    "        self.extractor = ModelLoader(model_path=reid_model_path, core=self.core, batchsize=-1)\n",
    "        self.frame_processor = FrameProcessor()\n",
    "        self.tracker = init_tracker()\n",
    "    \n",
    "\n",
    "    # Main processing function to run person tracking.\n",
    "    def run_person_tracking(self, source=0, flip=False, skip_first_frames=0):\n",
    "        \n",
    "        player = None\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            # Create a video player to play with target fps.\n",
    "            player = utils.VideoPlayer(\n",
    "                source=source, size=(700, 450), flip=flip, fps=24, skip_first_frames=skip_first_frames\n",
    "            )\n",
    "            # Start capturing.\n",
    "            player.start()\n",
    "            title = \"Person Tracking\"\n",
    "            cv2.namedWindow(winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "            processing_times = collections.deque()\n",
    "            processessed_ids = []\n",
    "\n",
    "            state_machine = PersonTrackingStateMachine()\n",
    "\n",
    "            while True:\n",
    "                clear_output(wait=True)  # Clear the previous output\n",
    "                # Grab the frame.\n",
    "                frame = player.next()\n",
    "                if frame is None:\n",
    "                    print(\"Source ended\")\n",
    "                    break\n",
    "                # If the frame is larger than full HD, reduce size to improve the performance.\n",
    "\n",
    "                # Resize the image and change dims to fit neural network input.\n",
    "                h, w = frame.shape[:2]\n",
    "                input_image = self.frame_processor.preprocess(frame, self.detector.height, self.detector.width)\n",
    "\n",
    "                # Measure processing time.\n",
    "                start_time = time.time()\n",
    "                # Get the results.\n",
    "                output = self.detector.predict(input_image)\n",
    "                stop_time = time.time()\n",
    "                processing_times.append(stop_time - start_time)\n",
    "                if len(processing_times) > 200:\n",
    "                    processing_times.popleft()\n",
    "\n",
    "                _, f_width = frame.shape[:2]\n",
    "                # Mean processing time [ms].\n",
    "                processing_time = np.mean(processing_times) * 1100\n",
    "                fps = 1000 / processing_time\n",
    "\n",
    "                # Get poses from detection results.\n",
    "                bbox_xywh, score, label = self.frame_processor.process_results(h, w, results=output)\n",
    "                \n",
    "                img_crops = []\n",
    "                for box in bbox_xywh:\n",
    "                    x1, y1, x2, y2 = xywh_to_xyxy(box, h, w)\n",
    "                    img = frame[y1:y2, x1:x2]\n",
    "                    img_crops.append(img)\n",
    "\n",
    "                # Get reidentification feature of each person.\n",
    "                if img_crops:\n",
    "                    # preprocess\n",
    "                    img_batch = self.frame_processor.batch_preprocess(img_crops, self.extractor.height, self.extractor.width)\n",
    "                    features = self.extractor.predict(img_batch)\n",
    "                else:\n",
    "                    features = np.array([])\n",
    "\n",
    "                # Wrap the detection and reidentification results together\n",
    "                bbox_tlwh = xywh_to_tlwh(bbox_xywh)\n",
    "                detections = [Detection(bbox_tlwh[i], features[i]) for i in range(features.shape[0])]\n",
    "\n",
    "                # predict the position of tracking target \n",
    "                self.tracker.predict()\n",
    "\n",
    "                # update tracker\n",
    "                self.tracker.update(detections)\n",
    "\n",
    "                # update bbox identities\n",
    "                outputs = []\n",
    "                \n",
    "                new_ids = []\n",
    "\n",
    "                for track in self.tracker.tracks:\n",
    "                    if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                        continue\n",
    "                    box = track.to_tlwh()\n",
    "                    x1, y1, x2, y2 = tlwh_to_xyxy(box, h, w)\n",
    "                    track_id = track.track_id\n",
    "                    outputs.append(np.array([x1, y1, x2, y2, track_id], dtype=np.int32))\n",
    "                    new_ids.append(track_id)\n",
    "\n",
    "                if len(outputs) > 0:\n",
    "                    outputs = np.stack(outputs, axis=0)\n",
    "\n",
    "                # draw box for visualization\n",
    "                if len(outputs) > 0:\n",
    "                    bbox_tlwh = []\n",
    "                    bbox_xyxy = outputs[:, :4]\n",
    "                    identities = outputs[:, -1]\n",
    "                    frame = self.frame_processor.draw_boxes(frame, bbox_xyxy, identities)\n",
    "\n",
    "                print(\"new_ids: \", new_ids)\n",
    "                state_machine.process_detections(new_ids)\n",
    "                state_machine.update_cooldown_timer()\n",
    "\n",
    "                cv2.putText(\n",
    "                    img=frame,\n",
    "                    text=f\"Inference time: {processing_time:.1f}ms\",\n",
    "                    org=(20, 30),\n",
    "                    fontFace=cv2.FONT_HERSHEY_PLAIN,\n",
    "                    fontScale=(f_width / 1000)*2,\n",
    "                    color=(0, 255, 0),\n",
    "                    thickness=2\n",
    "                )\n",
    "\n",
    "                cv2.putText(\n",
    "                    img=frame,\n",
    "                    text=f\"{fps:.1f} FPS\",\n",
    "                    org=(20, 60),\n",
    "                    fontFace=cv2.FONT_HERSHEY_PLAIN,\n",
    "                    fontScale=(f_width / 1000)*2,\n",
    "                    color=(0, 255, 0),\n",
    "                    thickness=2\n",
    "                )\n",
    "                \n",
    "                cv2.imshow(winname=title, mat=frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "                    \n",
    "            # Stop the sound thread before starting the next iteration\n",
    "            state_machine.stop_sound_thread()\n",
    "\n",
    "        \n",
    "        # ctrl-c\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupted\")\n",
    "        # any different error\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "        finally:\n",
    "            if player is not None:\n",
    "                # Stop capturing.\n",
    "                player.stop()\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(source, detection_model_path, reidentification_model_path):\n",
    "    obj = DetectionPipeline(detection_model_path, reidentification_model_path)\n",
    "    obj.run_person_tracking(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_ids:  [1]\n",
      "Cooldown timer: 4.54 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    detection_model_path = '/home/acer/workspace/intel_models/intel/person-detection-0202/FP16/person-detection-0202.xml'\n",
    "    reidentification_model_path = '/home/acer/workspace/intel_models/intel/person-reidentification-retail-0287/FP16/person-reidentification-retail-0287.xml'\n",
    "    \n",
    "    run(source=2, detection_model_path=detection_model_path, reidentification_model_path=reidentification_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.extend(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.extend(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
